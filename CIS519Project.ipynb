{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS519Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bheath015/Protein-Atlas-Image-Classification/blob/brian/CIS519Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p3iorOKShYo4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3W3Qhj1pC1ia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LumfhCpiew11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "# Imports\n",
        "############################################################\n",
        "# Include your imports here, if any are used.\n",
        "\n",
        "!pip install cnn_finetune\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from cnn_finetune import make_model\n",
        "import random\n",
        "import cv2\n",
        "import io, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTi4iXJwmkQt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download  -c human-protein-atlas-image-classification -p /content/kaggle\n",
        "\n",
        "!mkdir '/content/kaggle/train'\n",
        "!mkdir '/content/kaggle/test'\n",
        "\n",
        "!unzip -q /content/kaggle/train.zip -d '/content/kaggle/train'\n",
        "!unzip -q /content/kaggle/test.zip -d '/content/kaggle/test'\n",
        "\n",
        "os.listdir('/content/kaggle/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIURTSAq3u-W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r /content/kaggle/test/sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uX2lVGY2oN6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hex_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\n",
        "os.mkdir('/content/kaggle/test/sub/')\n",
        "for hex_val in hex_set:\n",
        "    os.mkdir('/content/kaggle/test/sub/{}'.format(hex_val))\n",
        "for file in os.listdir('/content/kaggle/test'):\n",
        "    if len(file) > 3:\n",
        "        os.rename('/content/kaggle/test/{}'.format(file), '/content/kaggle/test/sub/{}/{}'.format(file[0], file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlbFnhK41bRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hex_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\n",
        "os.mkdir('/content/kaggle/train/sub/')\n",
        "for hex_val in hex_set:\n",
        "    os.mkdir('/content/kaggle/train/sub/{}'.format(hex_val))\n",
        "for file in os.listdir('/content/kaggle/train'):\n",
        "    if len(file) > 3:\n",
        "        os.rename('/content/kaggle/train/{}'.format(file), '/content/kaggle/train/sub/{}/{}'.format(file[0], file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rwARmJBme0nR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_data(x_data_filepath, y_data_filepath):\n",
        "    X = np.load(x_data_filepath)\n",
        "    y = np.load(y_data_filepath)\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t_VUdT1zKSUb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def four_to_three_channels(four_channel_image, compression_type='dropout', colors=[]):\n",
        "    \"\"\"\n",
        "    Converts 4-channel image to 3-channel image for use in CNN\n",
        "\n",
        "    Args:\n",
        "        four_channel_image: 4x512x512 numpy array\n",
        "        optional arguments for what type of compression to use\n",
        "        compression_type = 'dropout' or 'combo'\n",
        "        colors = a list of either one color if dropout or two colors if combo\n",
        "    Returns:\n",
        "        dictionary: keys - each image_name in image_names\n",
        "                    values - set of labels for each image_name\n",
        "                    \n",
        "    \"\"\"\n",
        "    color_ref_dict = {'red': 0, 'blue': 1, 'green': 2, 'yellow': 3}\n",
        "    \n",
        "    if compression_type is 'dropout':\n",
        "        three_channel_image = np.delete(four_channel_image,(color_ref_dict[colors[0]]), axis=0)\n",
        "    elif compression_type is 'combo':\n",
        "        three_channel_image = np.zeros((3, four_channel_image.shape[1], four_channel_image.shape[2]))\n",
        "        index_to_fill = 0\n",
        "        print(three_channel_image)\n",
        "        for color, index in color_ref_dict.items():\n",
        "            if color not in colors:\n",
        "                three_channel_image[index_to_fill : four_channel_image.shape[1] : four_channel_image.shape[2]] = four_channel_image[index : four_channel_image.shape[1] : four_channel_image.shape[2]]\n",
        "                index_to_fill += 1\n",
        "        three_channel_image[2, :, :] = four_channel_image[color_ref_dict[colors[0]]] + four_channel_image[color_ref_dict[colors[1]]]\n",
        "        three_channel_image[2, :, :] = three_channel_image[2, :, :] / 2                                                                                  \n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized compression_type\")\n",
        "  \n",
        "    return three_channel_image\n",
        "\n",
        "# four_to_three_channels(np.asarray([[[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
        "#                         [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
        "#                         [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
        "#                         [[4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 4]]]), compression_type='combo', colors=['red', 'blue'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oa1JD6CKKTYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_label_dict(excel_file_path, image_names):\n",
        "    \"\"\"\n",
        "    Gets label for inputted image.\n",
        "\n",
        "    Args:\n",
        "        excel_file_path: file path to excel sheet with labels\n",
        "        image_names: list of names of images that have four color channel filters: red, blue, green and yellow\n",
        "            e.g. '7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0' is an image_name, with filter files: \n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_red.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_blue.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_green.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_yellow.png\n",
        "    Returns:\n",
        "        dictionary: keys - each image_name in image_names\n",
        "                    values - set of labels for each image_name\n",
        "                    \n",
        "    \"\"\"\n",
        "  \n",
        "  # loop through excel file and parse image_name and labels from each row\n",
        "  # for row in file:\n",
        "        image_name = None # some string\n",
        "        labels = None # some list\n",
        "        label_dict[image_name] = set(labels) \n",
        "  \n",
        "    return label_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bfk1oQvf39L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "############################################################\n",
        "# Extracting and loading data\n",
        "############################################################\n",
        "class Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Kaggle Human Protein Atlas Image Classification Dataset\n",
        "    Attributes:\n",
        "        image_names: list of names of images that have four color channel filters: red, blue, green and yellow\n",
        "            e.g. '7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0' is an image_name, with filter files: \n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_red.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_blue.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_green.png\n",
        "            7afe21e8-bbb8-11e8-b2ba-ac1f6b6435d0_yellow.png\n",
        "        len: number of images (groups of 4 filters: red, blue, green, yellow) in dataset \n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, data_path, excel_file_path, transformations=None, normalize=True):\n",
        "        \n",
        "        channel_colors = ['red', 'blue', 'green', 'yellow']\n",
        "  \n",
        "        # make a list of all 4-channel images in directory (red, blue, green, yellow)\n",
        "        self.image_names = list({image_name.split('_')[0] \n",
        "                                 for image_name in os.listdir(directory_name)})\n",
        "        \n",
        "        # set length of dataset\n",
        "        self.len = len(image_names) \n",
        "        \n",
        "        # create dictionary of labels for dataset\n",
        "        self.labels = make_label_dict(excel_file_path, image_names)\n",
        "        \n",
        "        # shuffle images\n",
        "        random.shuffle(image_names) \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # retrieve image name from list of all image names in dataset \n",
        "        image_name = image_names[idx]\n",
        "        \n",
        "        # create a list of all the channels in the image \n",
        "        channels = [cv2.imread(directory_name + '/' + image_name + '_' + color + \n",
        "                               '.png', 0) for color in channel_colors]\n",
        "\n",
        "        # check if any of the channels are missing for image\n",
        "        missing_channels = [x is None for x in channels]\n",
        "        if any(missing_channels):\n",
        "            raise FileNotFoundError('error:', image_name + ', missing', [channel_colors[i] for i, x in enumerate(channels) if x is None])\n",
        "        \n",
        "        # convert image into 4x520x520 numpy array by stacking the 4 filters along the 0th axis\n",
        "        np_image = np.stack(channels, axis=0)\n",
        "        \n",
        "        return np_image, self.labels[image_name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUzoAdc4djZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "# Convolutional Neural Network\n",
        "############################################################\n",
        "class ConvolutionalNN(nn.Module):\n",
        "    \"\"\" \n",
        "        (1) Use self.conv1 as the variable name for your first convolutional layer\n",
        "        (2) Use self.pool as the variable name for your pooling layer\n",
        "        (3) User self.conv2 as the variable name for your second convolutional layer\n",
        "        (4) Use self.fc1 as the variable name for your first fully connected layer\n",
        "        (5) Use self.fc2 as the variable name for your second fully connected layer\n",
        "        (6) Use self.fc3 as the variable name for your third fully connected layer\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(ConvolutionalNN, self).__init__()\n",
        "        \n",
        "        # Conv2D: (input channels, output channels, filter size, stride, padding)\n",
        "        self.conv1 = nn.Conv2d(3, 7, 3, 1, 0) \n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(7, 16, 3, 1, 0) \n",
        "\n",
        "        self.fc1 = nn.Linear(16*13*13, 130) \n",
        "        self.fc2 = nn.Linear(130, 72) \n",
        "        self.fc3 = nn.Linear(72, 10) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # tensor size (x, y, z, z) <- x: batch size, y: number of channels, z: dimension of each channel\n",
        "        \n",
        "        # out = (64, 7, 30, 30) <- 32x32 input with 3x3 filter: 30x30 output (32-(3-1)=30)  \n",
        "        out = F.relu(self.conv1(x))\n",
        "        \n",
        "        # out = (64, 7, 15, 15) <- 30x30 input with 2x2 filter: 15x15 output (30/2=15)\n",
        "        out = self.pool(out)\n",
        "        \n",
        "        # out = (64, 16, 13, 13) <- 13x13 input with 3x3 filter: 13x13 output (15-(3-1)=13)  \n",
        "        out = F.relu(self.conv2(out))\n",
        "\n",
        "        # 3 fully connected layers (16x13x13 -> 130 -> 72 -> 10)\n",
        "        out = out.view(out.size(0), -1) # flatten each example: out = (64, \") <- batch_size x (16x13x13)\n",
        "        out = F.sigmoid(self.fc1(out))\n",
        "        out = F.sigmoid(self.fc2(out)) \n",
        "        out = F.sigmoid(self.fc3(out))\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    \"\"\" \n",
        "        Please do not change the functions below. \n",
        "        They will be used to test the correctness of your implementation \n",
        "    \"\"\"\n",
        "    def get_conv1_params(self):\n",
        "        return self.conv1.__repr__()\n",
        "    \n",
        "    def get_pool_params(self):\n",
        "        return self.pool.__repr__()\n",
        "\n",
        "    def get_conv2_params(self):\n",
        "        return self.conv2.__repr__()\n",
        "    \n",
        "    def get_fc1_params(self):\n",
        "        return self.fc1.__repr__()\n",
        "    \n",
        "    def get_fc2_params(self):\n",
        "        return self.fc2.__repr__()\n",
        "    \n",
        "    def get_fc3_params(self):\n",
        "        return self.fc3.__repr__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "paHtQfk8XFaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def AdvancedCNN(model,num_classes=27,input_size=(512,512)):\n",
        "    CNN_model = make_model(model, num_classes=num_classes, pretrained=True, input_size=input_size)\n",
        "    \n",
        "    return CNN_model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yc0sIBiYdzFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "############################################################\n",
        "# Run Experiment\n",
        "############################################################\n",
        "def run_experiment(neural_network, train_loader, test_loader, loss_function, optimizer):\n",
        "    \"\"\"\n",
        "    Runs experiment on the model neural network given a train and test data loader, loss function and optimizer.\n",
        "\n",
        "    Args:\n",
        "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
        "                                                                FeedForwardNN or ConvolutionalNN,\n",
        "        train_loader (DataLoader),\n",
        "        test_loader (DataLoader),\n",
        "        loss_function (torch.nn.CrossEntropyLoss),\n",
        "        optimizer (optim.SGD)\n",
        "    Returns:\n",
        "        tuple: First position, testing accuracy.\n",
        "               Second position, training accuracy.\n",
        "               Third position, training loss.\n",
        "\n",
        "               For example, if you find that\n",
        "                            testing accuracy = 0.76,\n",
        "                            training accuracy = 0.24\n",
        "                            training loss = 0.56\n",
        "\n",
        "               This function should return (0.76, 0.24, 0.56)\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "       neural_network.cuda()\n",
        "    \n",
        "    max_epochs = 100\n",
        "    train_loss = np.zeros((max_epochs))\n",
        "    train_accuracy = np.zeros((max_epochs))\n",
        "    test_accuracy = np.zeros((max_epochs))\n",
        "    \n",
        "    # optimize weights\n",
        "    for epoch in range(max_epochs):\n",
        "    \n",
        "        print('Training neural network...')\n",
        "        for i, data in enumerate(train_loader, 0):            \n",
        "            train_batch_NN(data, neural_network, loss_function, optimizer) # train on batch\n",
        "\n",
        "        print('Calculating Statistics over Epochs:')\n",
        "        # get training loss and accuracy for this epoch  \n",
        "        train_loss[epoch], train_accuracy[epoch] = get_train_statistics(train_loader, neural_network, loss_function)\n",
        "        test_accuracy[epoch] = get_test_statistics(test_loader, neural_network)\n",
        "        print(\"epoch:\", epoch, \"loss:\", train_loss[epoch], \"train accuracy:\", train_accuracy[epoch], \"test accuracy:\", \n",
        "              test_accuracy[epoch])\n",
        "        \n",
        "    return (train_loss, train_accuracy, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYaxaZWresbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Normalizes the RGB pixel values of an image.\n",
        "\n",
        "    Args:\n",
        "        image (3D NumPy array): For example, it should take in a single 3x32x32 image from the CIFAR-10 dataset\n",
        "    Returns:\n",
        "        tuple: The normalized image\n",
        "    \"\"\"\n",
        "    mean = np.mean(image, axis=(1,2))\n",
        "    std = np.std(image, axis=(1,2))\n",
        "    normalized_image = ((image[0,:,:] - mean[0]) / std[0], \n",
        "                        (image[1,:,:] - mean[1]) / std[1], \n",
        "                        (image[2,:,:] - mean[2]) / std[2])\n",
        "    return np.stack(normalized_image, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M6T8PTvWe8uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# REQUIRED METHOD - Function to train a single batch in the NN (used in run_experiment())\n",
        "\n",
        "def train_batch_NN(data, neural_network, loss_function, optimizer):\n",
        "\n",
        "    # Get inputs and labels from data loader \n",
        "    inputs, labels = data\n",
        "\n",
        "    #print(inputs.size())\n",
        "    inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      inputs.cuda()\n",
        "      labels.cuda()\n",
        "\n",
        "\n",
        "    # Feed the input data into the network \n",
        "    y_pred = neural_network(inputs)\n",
        "\n",
        "\n",
        "    # Calculate the loss using predicted labels and ground truth labels\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = loss_function(y_pred, labels)\n",
        "\n",
        "\n",
        "    # backpropogates to compute gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # updates the weghts\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yrc0ynafCIv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# REQUIRED METHOD - Function to get loss and accuracy across all training examples in the NN (used in run_experiment())\n",
        "\n",
        "def get_train_statistics(train_loader, neural_network, loss_function):\n",
        "    \n",
        "    correct = 0\n",
        "    sum_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "          \n",
        "            # Get inputs and labels from data loader \n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "              inputs.cuda()\n",
        "              labels.cuda()\n",
        "            \n",
        "        \n",
        "            # Feed the input data into the network \n",
        "            y_pred = neural_network(inputs)\n",
        "            \n",
        "            # Calculate the loss using predicted labels and ground truth labels\n",
        "            loss = loss_function(y_pred, labels)\n",
        "\n",
        "            \n",
        "            # convert predicted labels into numpy\n",
        "            nploss= loss.cpu().data.numpy()\n",
        "            sum_loss = sum_loss + nploss\n",
        "            \n",
        "            \n",
        "            y_pred_np = y_pred.cpu().data.numpy()\n",
        "            label_np = labels.cpu().data.numpy().reshape(len(labels),1)\n",
        "            \n",
        "              \n",
        "            pred_np = np.argmax(y_pred_np, axis=1)\n",
        "            \n",
        "            for j in range(y_pred_np.shape[0]):\n",
        "                if pred_np[j] == label_np[j]:\n",
        "                    correct += 1\n",
        "    \n",
        "    # calculate the accuracy across batches\n",
        "    train_accuracy = float(correct)/float(len(train_loader.dataset))\n",
        "    \n",
        "    return sum_loss, train_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULpwmUC4fFfE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# REQUIRED METHOD - Function to accuracy across all test examples in the NN (used in run_experiment())\n",
        "\n",
        "def get_test_statistics(test_loader, neural_network):\n",
        "    \n",
        "    correct_test = 0\n",
        "    \n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "\n",
        "            # Get inputs and labels from data loader \n",
        "            inputs, labels = data\n",
        "\n",
        "            #print(inputs.size())\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "              inputs.cuda()\n",
        "              labels.cuda()\n",
        "\n",
        "\n",
        "            # Feed the input data into the network \n",
        "            y_pred_test = neural_network(inputs)\n",
        "\n",
        "\n",
        "            # convert predicted laels into numpy\n",
        "            y_pred_np_test = y_pred_test.cpu().data.numpy()\n",
        "            label_np_test = labels.cpu().data.numpy().reshape(len(labels),1)\n",
        "           \n",
        "          \n",
        "            # calculate the training accuracy of the current model\n",
        "            pred_np_test = np.argmax(y_pred_np_test, axis=1)\n",
        "\n",
        "\n",
        "            for j in range(y_pred_np_test.shape[0]):\n",
        "                if pred_np_test[j] == label_np_test[j]:\n",
        "                    correct_test += 1\n",
        "    \n",
        "    # calculate the accuracy across batches\n",
        "    test_accuracy = float(total_correct) / float(len(test_loader.dataset))\n",
        "    \n",
        "    return test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BbMZi746fIeA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to plot the loss and training accuracy over epochs\n",
        "\n",
        "def plot_epoch_stats(loss, train_accuracy, test_accuracy):\n",
        "    \n",
        "    max_epochs = 100\n",
        "    print(\"final training accuracy: \", train_accuracy[max_epochs-1])\n",
        "    print(\"final test accuracy: \", test_accuracy[max_epochs-1])\n",
        "    epoch_number = np.arange(0,max_epochs,1)\n",
        "\n",
        "    # Plot the loss over epoch\n",
        "    plt.figure()\n",
        "    plt.plot(epoch_number, loss)\n",
        "    plt.title('Total Loss over Epochs')\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Plot the training accuracy over epoch\n",
        "    plt.figure()\n",
        "    plt.plot(epoch_number, train_accuracy)\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    \n",
        "    # Plot the testing accuracy over epoch\n",
        "    plt.figure()\n",
        "    plt.plot(epoch_number, test_accuracy)\n",
        "    plt.title('Testing Accuracy over Epochs')\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6upZfsfHfY6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RUN CODE BELOW HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ah79C1O5wh18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# function to load all images in directory as numpy array\n",
        "\n",
        "def data_to_numpy(directory_name, num_images=None):\n",
        "  '''\n",
        "  Construct a num_imagesxwidthxheight numpy array of \n",
        "\n",
        "  :param directory_name: name of the directory where the dataset is stored\n",
        "  :param num_images: number of images to keep in resulting 4D numpy array\n",
        "                     if None, use all images\n",
        "  :return: numpy array of images: (num_images, 4 color channels, width, height)\n",
        "  '''\n",
        "  \n",
        "  channel_colors = ['red', 'blue', 'green', 'yellow']\n",
        "  \n",
        "  # make a list of all 4-channel images in directory (red, blue, green, yellow)\n",
        "  image_names = list({image_name.split('_')[0] \n",
        "                      for image_name in os.listdir(directory_name) if image_name.split('.')[-1] == 'png'})\n",
        "  random.shuffle(image_names) # shuffle images\n",
        "    \n",
        "  # if num_images is None, set to use ALL images in directory\n",
        "  if num_images == None:\n",
        "    num_images = len(image_names)\n",
        "  \n",
        "  # put 4-channel images into 4D numpy array\n",
        "  images = []\n",
        "  \n",
        "  count = 0\n",
        "  num_used = 0\n",
        "  while num_used < num_images and count < len(image_names):\n",
        "    \n",
        "    image_name = image_names[count]\n",
        "    count += 1\n",
        "      \n",
        "    # create a list of all the channels in the image \n",
        "    channels = [cv2.imread(directory_name + '/' + image_name + '_' + color + \n",
        "                           '.png', 0) for color in channel_colors]\n",
        "\n",
        "    # check if any of the channels are missing for image\n",
        "    missing_channels = [x is None for x in channels]\n",
        "    \n",
        "    # add 3D image to list if no channels are missing\n",
        "    if any(missing_channels):\n",
        "      print('skipped', image_name + ', missing',  ###### COMMENT THIS OUT\n",
        "            [channel_colors[i] for i, x in enumerate(channels) if x is None])\n",
        "      continue\n",
        "    else:\n",
        "#       print('used', image_name) ###### COMMENT THIS OUT\n",
        "      num_used += 1\n",
        "      images.append(np.stack(channels, axis=0))\n",
        "  \n",
        "  # stack 3D images into 4D numpy array\n",
        "  print('skipped', count - num_used, 'images due to missing channels')\n",
        "  return np.stack(images, axis=0), image_names\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqJXk0utTgqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hex_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\n",
        "for hex_val in hex_set:\n",
        "    num_files = None\n",
        "    [x_test, test_images] = data_to_numpy('/content/kaggle/test/sub/{}'.format(hex_val), num_files)\n",
        "    print(x_test.shape)\n",
        "    np.save(\"/content/kaggle/test/X_test{}.npy\".format(hex_val), x_test)\n",
        "    np.save(\"/content/kaggle/test/test_images{}\".format(hex_val), test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7kctwYl0nFg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "hex_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\n",
        "print(os.listdir('/content/kaggle/test/'))\n",
        "for hex_val in hex_set:\n",
        "    if \"X_test{}.npy\".format(hex_val) in os.listdir('/content/kaggle/test/'):\n",
        "        copyfile(\"/content/kaggle/test/X_test{}.npy\".format(hex_val), '/content/drive/My Drive/Colab Notebooks/atlas-test/X_test{}.npy'.format(hex_val))\n",
        "        copyfile('/content/kaggle/test/test_images{}.npy'.format(hex_val), '/content/drive/My Drive/Colab Notebooks/atlas-test/test_images{}.npy'.format(hex_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbNV2dm21dTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for hex_val in hex_set:\n",
        "    num_files = None\n",
        "    [x_train, train_images] = data_to_numpy('/content/kaggle/train/sub/{}'.format(hex_val), num_files)\n",
        "    print(x_train.shape)\n",
        "    np.save(\"/content/kaggle/train/X_test{}.npy\".format(hex_val), x_train)\n",
        "    np.save(\"/content/kaggle/train/test_images{}\".format(hex_val), train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ol7eXWsBQQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.listdir('/content/kaggle/train/')\n",
        "from shutil import copyfile\n",
        "hex_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\n",
        "for hex_val in hex_set:\n",
        "    if \"X_test{}.npy\".format(hex_val) in os.listdir('/content/kaggle/train/'):\n",
        "        copyfile(\"/content/kaggle/train/X_test{}.npy\".format(hex_val), '/content/drive/My Drive/Colab Notebooks/atlas-train/X_train{}.npy'.format(hex_val))\n",
        "        copyfile('/content/kaggle/train/test_images{}.npy'.format(hex_val), '/content/drive/My Drive/Colab Notebooks/atlas-train/train_images{}.npy'.format(hex_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33qJ3E7Or5Tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2223
        },
        "outputId": "b5530dfb-9004-4555-ec75-eff5b355ab3c"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import pandas as pd\n",
        "\n",
        "label_dict = {\n",
        "    0:  \"Nucleoplasm\",  \n",
        "    1:  \"Nuclear membrane\",   \n",
        "    2:  \"Nucleoli\",   \n",
        "    3:  \"Nucleoli fibrillar center\",   \n",
        "    4:  \"Nuclear speckles\",\n",
        "    5:  \"Nuclear bodies\",   \n",
        "    6:  \"Endoplasmic reticulum\",   \n",
        "    7:  \"Golgi apparatus\",   \n",
        "    8:  \"Peroxisomes\",   \n",
        "    9:  \"Endosomes\",   \n",
        "    10:  \"Lysosomes\",   \n",
        "    11:  \"Intermediate filaments\",   \n",
        "    12:  \"Actin filaments\",   \n",
        "    13:  \"Focal adhesion sites\",   \n",
        "    14:  \"Microtubules\",   \n",
        "    15:  \"Microtubule ends\",   \n",
        "    16:  \"Cytokinetic bridge\",   \n",
        "    17:  \"Mitotic spindle\",   \n",
        "    18:  \"Microtubule organizing center\",   \n",
        "    19:  \"Centrosome\",   \n",
        "    20:  \"Lipid droplets\",   \n",
        "    21:  \"Plasma membrane\",   \n",
        "    22:  \"Cell junctions\",   \n",
        "    23:  \"Mitochondria\",   \n",
        "    24:  \"Aggresome\",   \n",
        "    25:  \"Cytosol\",   \n",
        "    26:  \"Cytoplasmic bodies\",   \n",
        "    27:  \"Rods & rings\"\n",
        "}\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "train_labels_file_path = '/content/kaggle/train.csv'\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "with open(train_labels_file_path) as train_labels_file:\n",
        "    labels_df = pd.read_csv(train_labels_file)\n",
        "\n",
        "rows = labels_df.shape[0]\n",
        "for i in range(rows):\n",
        "    labels_df['Target'][i] = labels_df['Target'][i].split(' ')\n",
        "\n",
        "out = pd.DataFrame(mlb.fit_transform(labels_df['Target']), columns=mlb.classes_, index=labels_df.index)\n",
        "print(out)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       0  1  10  11  12  13  14  15  16  17 ...  25  26  27  3  4  5  6  7  8  \\\n",
            "0      1  0   0   0   0   0   0   0   1   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "1      1  1   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  1  0   \n",
            "2      0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  1  0  0  0   \n",
            "3      0  1   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "4      0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "5      1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "6      0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "7      1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "8      1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "9      0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  1  0   \n",
            "10     0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "11     0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "12     1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "13     0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "14     0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  1  0  0   \n",
            "15     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "16     1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "17     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "18     1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "19     0  0   0   1   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "20     1  1   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  1  0   \n",
            "21     0  0   0   1   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "22     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "23     1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "24     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  1  0  0  0  0  0   \n",
            "25     0  0   0   0   0   0   0   0   1   0 ...   0   0   0  0  0  0  1  0  0   \n",
            "26     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "27     0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "28     0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "29     1  0   0   0   1   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "...   .. ..  ..  ..  ..  ..  ..  ..  ..  .. ...  ..  ..  .. .. .. .. .. .. ..   \n",
            "31042  1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31043  1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31044  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31045  1  0   0   1   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31046  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31047  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  1  0   \n",
            "31048  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  1  0  0  0  0   \n",
            "31049  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  1  0  0  0  0  0   \n",
            "31050  0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  1  0   \n",
            "31051  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31052  0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  1  0   \n",
            "31053  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31054  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  1  0  0  0   \n",
            "31055  1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31056  1  0   0   0   0   0   0   0   0   0 ...   1   0   0  1  0  0  0  0  0   \n",
            "31057  0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31058  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  1  0  0  0  0   \n",
            "31059  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31060  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31061  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31062  0  0   0   0   0   0   1   0   1   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31063  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  1  0  0  0   \n",
            "31064  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31065  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31066  1  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31067  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "31068  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  1  0  0  0   \n",
            "31069  0  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  1  0   \n",
            "31070  0  0   0   0   0   0   0   0   0   0 ...   1   0   0  0  0  0  0  0  0   \n",
            "31071  1  0   0   0   0   0   0   0   0   0 ...   0   0   0  0  0  0  0  0  0   \n",
            "\n",
            "       9  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      0  \n",
            "5      0  \n",
            "6      0  \n",
            "7      0  \n",
            "8      0  \n",
            "9      0  \n",
            "10     0  \n",
            "11     0  \n",
            "12     0  \n",
            "13     0  \n",
            "14     0  \n",
            "15     0  \n",
            "16     0  \n",
            "17     0  \n",
            "18     0  \n",
            "19     0  \n",
            "20     0  \n",
            "21     0  \n",
            "22     0  \n",
            "23     0  \n",
            "24     0  \n",
            "25     0  \n",
            "26     0  \n",
            "27     0  \n",
            "28     0  \n",
            "29     0  \n",
            "...   ..  \n",
            "31042  0  \n",
            "31043  0  \n",
            "31044  0  \n",
            "31045  0  \n",
            "31046  0  \n",
            "31047  0  \n",
            "31048  0  \n",
            "31049  0  \n",
            "31050  0  \n",
            "31051  0  \n",
            "31052  0  \n",
            "31053  0  \n",
            "31054  0  \n",
            "31055  0  \n",
            "31056  0  \n",
            "31057  0  \n",
            "31058  0  \n",
            "31059  0  \n",
            "31060  0  \n",
            "31061  0  \n",
            "31062  0  \n",
            "31063  0  \n",
            "31064  0  \n",
            "31065  0  \n",
            "31066  0  \n",
            "31067  0  \n",
            "31068  0  \n",
            "31069  0  \n",
            "31070  0  \n",
            "31071  0  \n",
            "\n",
            "[31072 rows x 28 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GlGMmUOBfdwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize constants\n",
        "'''\n",
        "train_images_file_path = '/content/gdrive/My Drive/Colab Notebooks/train'\n",
        "test_images_file_path = '/content/gdrive/My Drive/Colab Notebooks/test'\n",
        "test_labels_file_path = '/content/gdrive/My Drive/Colab Notebooks/cifar10-data/test_labels.npy'\n",
        "\n",
        "# create train and test data loaders\n",
        "X_train, y_train = extract_data('gdrive/My Drive/?.npy', 'gdrive/My Drive/?.npy') # ? examples ################################ GPU\n",
        "X_test, y_test = extract_data('gdrive/My Drive/?.npy', 'gdrive/My Drive/?.npy') # ? examples ################################ GPU\n",
        "'''\n",
        "# specify the loss function\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "if torch.cuda.is_available():\n",
        "   loss_function.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbwVA0iWcWvU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "x_train = 'drive/My Drive/COLAB/CIS 519/HW3/cifar10-data/train_images.npy'\n",
        "y_train = 'drive/My Drive/COLAB/CIS 519/HW3/cifar10-data/train_labels.npy'\n",
        "X_train,y_train = extract_data(x_train,y_train)\n",
        "\n",
        "\n",
        "  \n",
        "x_test = 'drive/My Drive/COLAB/CIS 519/HW3/cifar10-data/test_images.npy'\n",
        "y_test= 'drive/My Drive/COLAB/CIS 519/HW3/cifar10-data/test_labels.npy'\n",
        "X_test, y_test = extract_data(x_test,y_test)\n",
        "\n",
        "train_dataset = Dataset(X_train, y_train, normalize=False)\n",
        "test_dataset = Dataset(X_test, y_test, normalize=False)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQBFjTeEfveA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run Experiment and save results\n",
        "\n",
        "train_dataset = Dataset(X_train, y_train, normalize=False)\n",
        "test_dataset = Dataset(X_test, y_test, normalize=False)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "conv_raw_net = ConvolutionalNN() # create the NN\n",
        "optimizer = optim.SGD(conv_raw_net.parameters(), lr=0.001, momentum=0.9) # specify the optimizer\n",
        "(loss, train_accuracy, test_accuracy) = run_experiment(conv_raw_net, train_loader, test_loader, loss_function, optimizer)\n",
        "\n",
        "# np.save(\"gdrive/My Drive/statistics/conv_loss.npy\", loss)\n",
        "# np.save(\"gdrive/My Drive/statistics/conv_train_accuracy.npy\", train_accuracy)\n",
        "# np.save(\"gdrive/My Drive/statistics/conv_test_accuracy.npy\", test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQtZjyOqYtbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run Experiment on Advanced CNN:\n",
        "\n",
        "# get the name of model here https://pypi.org/project/cnn-finetune/\n",
        "\n",
        "name = 'resnet18'\n",
        "CNN_Model = AdvancedCNN(name)\n",
        "optimizer = optim.SGD(CNN_Model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "(loss, train_accuracy, test_accuracy) = run_experiment(CNN_Model, train_loader, test_loader, loss_function, optimizer)\n",
        "plot_epoch_stats(loss, train_accuracy, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}